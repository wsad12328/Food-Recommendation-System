\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/Model.pdf} % 替換成你的圖片檔案
    \caption{模型架構圖}
    \label{Model}
\end{figure*}

\section{Proposed}
\subsection{Single Embedding Layer}
    本研究所採用的~NIE-GCN\cite{NIE-GCN}~模型可拆解成幾個主要步驟如\xfig{Model}，首先是對餐廳節點進行向量嵌入。第一步會針對每個餐廳節點 $i$ 生成一個嵌入向量 $e_i$，此向量位於 $d$ 維空間中，即 $e_i \in \mathbb{R}^d$。在嵌入過程中，所有餐廳節點的嵌入向量會被收集成一組矩陣 $E_I^{(0)}$，此矩陣表達了所有餐廳節點的初始嵌入狀態，如下式所示： 
    \begin{equation} 
        E_I^{(0)} = {e_{i_1}^{(0)}, e_{i_2}^{(0)},\cdots,e_{i_N}^{(0)}} \in \mathbb{R}^{N \times d}, 
    \end{equation}
    其中，$N$ 代表餐廳節點的總數，而 $d$ 則是嵌入向量的維度；上標 $0$ 表示第 0 層傳播的初始狀態。藉由得到每個餐廳節點的嵌入向量後，模型便可利用這些向量作為基礎，來進一步推斷每位使用者的偏好。因為每個使用者節點 $u$ 的鄰居必定為餐廳節點，故模型會透過與該使用者互動過的餐廳節點來推敲其個人喜好。

    在完成餐廳節點的嵌入後，第二步則是建構每位使用者節點的初始嵌入向量 $e_u^{(0)}$。此過程需要依賴與使用者節點 $u$ 相鄰的餐廳節點 $i$ 的嵌入向量 $e_i^{(0)}$。第 0 層的嵌入構造方式如下： 
    \begin{equation} 
        e_u^{(0)} = \sigma \left(\sum_{i \in N_u} \frac{1}{\sqrt{\vert N_u \vert \vert N_i \vert}}e_i^{(0)}\right), 
        \label{eq-e_u^0}
    \end{equation} 
    其中 $N_u$ 表示使用者節點 $u$ 的鄰居集合，而 $N_i$ 則代表餐廳節點 $i$ 的鄰居集合；$\sigma(\cdot)$ 是激活函數，選擇使用雙曲正切函數 (tanh)。透過這一層的加權平均，可以有效地融合使用者與其鄰近餐廳節點的特徵信息，以更準確地反映該使用者的行為特徵。

\subsection{Propagation Layers}
    完成初始嵌入後，下一步是計算每個使用者節點 $u$ 與其相鄰的餐廳節點 $i$ 之間的注意力分數 $\rho(u, i)$，藉此衡量不同鄰居節點的重要性。該分數計算方式如下： 
    \begin{equation} 
        \rho(u, i) = Q^T\sigma(W(e_u^{(0)}||(e_i^{(0)})+b)), 
    \end{equation} 
    其中 $W \in \mathbb{R}^{2d \times d}$、$Q \in \mathbb{R}^{1 \times d}$、$b \in \mathbb{R}^{1 \times d}$，這三個參數分別為注意力機制中的權重矩陣與偏置項。此處的 $\sigma(\cdot)$ 同樣為雙曲正切函數 (tanh)，而 $||$ 則表示向量的串接操作。$\rho(u, i)$ 的分數越高，代表使用者 $u$ 與餐廳 $i$ 之間的關聯性越強。因此，透過該分數可以量化每個餐廳節點對於預測使用者偏好的貢獻度。

    為將注意力分數限制在 $[0,1]$ 的範圍內，利用 Softmax 函數對這些分數進行範圍歸一化處理，得到最終的注意力值 $\alpha(u, i)$。公式如下： \begin{equation} \alpha(u, i) = \frac{\exp(\rho(u, i))}{\left(\sum_{j \in N_u}\exp(\rho(u, j))\right)^{\beta}}, \end{equation} 其中 $\alpha(u, i)$ 代表了使用者節點 $u$ 與餐廳節點 $i$ 之間的最終注意力值。透過~$\alpha(u, i)$，每個使用者節點均能依據其鄰居的特徵，綜合考量注意力分數來完成向量更新。


    在初始嵌入狀態完成後，為進行更深層的特徵傳播，使用者節點 $u$ 的嵌入向量 $e_u^{(k)}$ 將在每一層根據其鄰居節點的資訊進行更新。使用者的嵌入向量 $e_u^{(k)}$ 的更新方式會依據先前計算出的注意力權重 $\alpha(u, i)$，並利用相鄰餐廳節點 $i$ 在上一層的嵌入向量 $e_i^{(k-1)}$ 進行加法聚合 (Add Aggregation)。此聚合方式可以有效融合鄰居的特徵信息，更新的計算公式如下： 
    \begin{equation} 
        e_u^{(k)} = \sigma\left(\sum_{i \in N_u} \alpha(u, i)e_i^{(k-1)}\right), 
    \end{equation} 
    其中，$\sigma(\cdot)$ 為雙曲正切函數 (tanh)，$N_u$ 表示使用者節點 $u$ 的鄰居集合。透過加權聚合，使用者節點可以自適應地調整對不同鄰居特徵的重視程度，進一步增強模型對於不同使用者偏好的捕捉能力。

    同樣地，初始狀態後的餐廳節點的嵌入向量 $e_i^{(k)}$ 也會根據其相鄰使用者節點的嵌入進行更新。這個更新方式與\xeq{eq-e_u^0}~類似，餐廳節點 $i$ 的嵌入向量會根據其相鄰的使用者節點 $u$ 的嵌入向量 $e_u^{(k)}$ 進行加權平均，計算方式如下： 
    \begin{equation} e_i^{(k)} = \sigma \left(\sum_{u \in N_i} \frac{1}{\sqrt{\vert N_u \vert \vert N_i \vert}} e_u^{(k)}\right), 
        \label{eq-e_i} 
    \end{equation} 其中，$N_u$ 和 $N_i$ 分別表示使用者節點 $u$ 和餐廳節點 $i$ 的鄰居集合，並透過加權平均來控制每個鄰居對於嵌入向量的貢獻。此\xeq{eq-e_i}~與\xeq{eq-e_u^0}~中的加權項 $\frac{1}{\sqrt{\vert N_u \vert \vert N_i \vert}}$ 可以平衡不同鄰居數量對嵌入更新的影響，從而避免由於鄰居數量不均而引起的偏差。藉由此加權項，每個餐廳節點的嵌入向量都將根據與其相鄰使用者的特徵進行有效更新，從而更準確地反映餐廳與使用者間的潛在關係。

\subsection{Prediction Layers}
    在完成每一層使用者節點的嵌入向量 $e_u^{(k)}$ 和餐廳節點的嵌入向量 $e_i^{(k)}$ 的計算後，接下來的步驟是將這些嵌入向量進行聚合，以獲得最終的嵌入表達，在本研究將所有層的使用者嵌入向量和餐廳嵌入向量分別相加，計算公式如下： 
    \begin{equation} 
        e_u^* = \sum_{k=1}^{L} e_u^{(k)}, \quad e_i^* = \sum_{k=1}^{L} e_i^{(k)}, 
    \end{equation} 
    其中 $L$ 表示嵌入層的總數。這樣計算得到的 $e_u^*$ 和 $e_i^*$ 分別代表了使用者節點和餐廳節點的最終嵌入向量，這兩個向量綜合了多層的信息，能夠更全面地捕捉使用者與餐廳之間的潛在關聯。
    為了進一步評估使用者 $u$ 和餐廳 $i$ 之間的關聯性，將這兩個最終嵌入向量進行內積操作，為了將內積值限制在 0 到 1 的範圍內，引入了 S 型函數 (sigmoid function)，計算公式如下： 
    \begin{equation} 
        \hat{y_{ui}} = f(u, i) = \sigma(e_u^{*T} e_i^*), 
    \end{equation} 
    其中，$\hat{y_{ui}}$ 表示模型對於使用者 $u$ 和餐廳 $i$ 之間關聯性的預測結果。S 型函數 $\sigma(\cdot)$ 將內積的結果映射到 $[0, 1]$ 的範圍，使得模型的輸出可以被解釋為一個機率值，表達使用者 $u$ 對餐廳 $i$ 的喜好程度。

\subsection{Model Optimization}
    本研究模型的目標函數採用貝氏個性化排序 (Bayesian Personalized Ranking, BPR)，以實現使用者偏好排序的優化，具體公式如下：
    \begin{equation}
        \mathcal{L} = \sum_{(u, i) \in \mathcal{O^+}, (u, j) \in \mathcal{O^-}} -\ln\sigma(\hat{y_{ui}} - \hat{y_{uj}}) + \lambda\lVert \Theta \rVert_2^2,
    \end{equation}
    其中~$\mathcal{O^+}$ 和 $\mathcal{O^-}$~分別表示正樣本與負樣本的集合；正樣本為符合使用者偏好的項目，而負樣本為非偏好項目。$\sigma(\cdot)$ 是 S 型函數 (sigmoid function)，可將輸出的範圍限制在 $[0, 1]$ 之間，用以衡量正樣本和負樣本之間的預測差距。此目標函數的設計旨在使正樣本的預測值 $\hat{y_{ui}}$ 高於負樣本的預測值 $\hat{y_{uj}}$，即期望 $\hat{y_{ui}}$ 越高越好，而 $\hat{y_{uj}}$ 越低越好，從而增強模型對正負樣本的區分能力。

    該目標函數包含了一項 L2 正則項，用於減少模型過擬合的風險。$\lambda$ 是 L2 正則化的係數，$\Theta = \{E_I, Q, W, b\}$ 表示模型中所有需要學習的參數集。通過引入正則項，模型的權重得以控制在合理範圍內，避免權重過大導致的模型複雜性增加。

    在計算過程中，預測差距 $\hat{y_{ui}} - \hat{y_{uj}}$ 的值範圍在 $[-1, 1]$，經過 S 型函數 $\sigma(\cdot)$ 映射後，範圍被壓縮到 $[0, 1]$。為了使目標函數在最大化的情況下實現梯度下降 (gradient descent) 訓練，對公式添加負號，這樣在取對數 $\ln$ 後，所有值為正，因此相當於最小化正的損失值，使得模型能夠通過梯度下降的方式進行有效優化。
